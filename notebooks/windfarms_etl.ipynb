{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import geopandas as gpd\n",
    "import plotly_express as px\n",
    "import networkx as nx\n",
    "from networkx.exception import NetworkXNoPath\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Load Aleph entities\n",
    "2. Load Mapstand data\n",
    "3. Add missing geometries to mapstand, drop rows that are not used\n",
    "4. Merge Mapstand with Aleph entities\n",
    "5. Clean it up\n",
    "6. Analyse country information and shapes\n",
    "7. Analyse company information and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ALEPH = os.environ.get('PATH_WIND_ALEPH')\n",
    "PATH_RAW = os.environ.get('PATH_RAW')\n",
    "PATH_WIND = os.environ.get('PATH_WIND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(entities: List) -> pd.DataFrame:\n",
    "    '''Parses Aleph JSON data\n",
    "    '''\n",
    "    \n",
    "    entity_list = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        data = entity.get('properties')\n",
    "\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                data.update({key: ','.join(value)})\n",
    "        entity_id = {'id': entity.get('id')}\n",
    "        data.update(entity_id)\n",
    "        entity_list.append(data)\n",
    "    \n",
    "    df = pd.DataFrame(entity_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_entities(path: str, entity: str) -> pd.DataFrame:\n",
    "    '''Load entities from Aleph\n",
    "    (downloaded through alephclient)'''\n",
    "\n",
    "    entities = []\n",
    "    with open(f'{path}{entity}.json', 'r') as file:\n",
    "        for line in file:\n",
    "            entities.append(json.loads(line))\n",
    "\n",
    "    df = parse_json(entities)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Aleph entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import companies\n",
    "\n",
    "companies = load_entities(PATH_ALEPH, 'companies')\n",
    "companies.drop(['notes', 'summary', 'sourceUrl','publisher', 'alias', 'description', \n",
    "                'leiCode', 'parent', 'amountEur'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Import and clean assets\n",
    "\n",
    "assets = load_entities(PATH_ALEPH, 'assets')\n",
    "assets.dropna(subset='description', inplace=True)\n",
    "assets.drop(['title', 'authority', 'contractDate', 'jurisdiction', 'registrationNumber',\n",
    "             'previousName', 'parent', 'leiCode', 'sourceUrl', 'publisher'], axis=1, inplace=True)\n",
    "\n",
    "# Import ownerships\n",
    "\n",
    "ownerships = load_entities(PATH_ALEPH, 'ownerships')\n",
    "\n",
    "# Import legal entities\n",
    "\n",
    "legalentities = load_entities(PATH_ALEPH, 'legalentities')\n",
    "legalentities.drop(['idNumber', 'notes',\n",
    "                    'publisherUrl', 'nationality', 'jurisdiction', 'previousName',\n",
    "                    'registrationNumber', 'sourceUrl', 'summary', 'publisher', 'alias',\n",
    "                    'description', 'leiCode', 'parent', 'amountEur'], axis=1, inplace=True)\n",
    "\n",
    "# Import contracts\n",
    "contracts = load_entities(PATH_ALEPH, 'contracts')\n",
    "\n",
    "# Import contract awards\n",
    "\n",
    "contractawards = load_entities(PATH_ALEPH, 'contractawards')\n",
    "\n",
    "# Import people\n",
    "\n",
    "persons = load_entities(PATH_ALEPH, 'persons')\n",
    "\n",
    "# Import other links\n",
    "\n",
    "other = load_entities(PATH_ALEPH, 'otherlinks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import geometries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import geojson\n",
    "\n",
    "gdf = gpd.read_file(PATH_WIND + 'gis/wind_farms_20231012.geojson')\n",
    "\n",
    "gdf = gdf.to_crs(28992)\n",
    "\n",
    "# Drop columns\n",
    "\n",
    "gdf.drop(['installed_capacity_mw', 'mps_est_shore_status', 'mps_est_area_sqkm', 'mps_id', 'capacity'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "\n",
    "cols = {'simple_status': 'status_simplified',\n",
    "        'mps_id': 'mps_uuid'}\n",
    "\n",
    "gdf = gdf.rename(columns=cols)\n",
    "\n",
    "# There is some data missing\n",
    "\n",
    "gdf.loc[gdf.mps_uuid=='f1328c2b-4821-4caf-b861-dfbd313c5ca2', 'year'] = 2028\n",
    "gdf.loc[((gdf.country=='nl') & (gdf.status.isna())), 'status'] = 'EARLY_PLANNING'\n",
    "gdf.loc[((gdf.country=='nl') & (gdf.status.isna())), 'status_simplified'] = 'EARLY_STAGE_PLANS'\n",
    "gdf.loc[gdf.mps_uuid=='b1744eed-ee27-4b7a-be61-06fea1a3082e', 'status'] = 'CONSENT_AUTHORISED'\n",
    "gdf.loc[gdf.mps_uuid=='b1744eed-ee27-4b7a-be61-06fea1a3082e', 'status_simplified'] = 'LATE_STAGE_PLANS'\n",
    "gdf.loc[gdf.mps_uuid=='7581a6a1-a8e3-4d12-9f1c-79a89d73f685', 'status_simplified'] = 'EARLY_STAGE_PLANS'\n",
    "gdf.loc[gdf.mps_uuid=='7581a6a1-a8e3-4d12-9f1c-79a89d73f685', 'status'] = 'EARLY_PLANNING'\n",
    "\n",
    "# Clean dtypes\n",
    "\n",
    "gdf['year'] = gdf.installation_year.apply(lambda x: pd.to_datetime(f\"{str(x).replace('.0', '')}-01-01\") if str(x) !='nan' else x)\n",
    "gdf.drop('installation_year', axis=1, inplace=True)\n",
    "cols = ['capacity_mw', 'value_eur', 'mps_est_elevation_min_m', 'mps_est_elevation_max_m', 'number_generators']\n",
    "for col in cols:\n",
    "        gdf[col] = np.floor(pd.to_numeric(gdf[col], errors='coerce')).astype('Int64')\n",
    "\n",
    "# Create km2 area column\n",
    "\n",
    "gdf['area_km'] = gdf.geometry.area / 1000000\n",
    "gdf = gdf.to_crs(4326)\n",
    "len(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(PATH_WIND + 'gis/windfarms_v1.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process ownership structure\n",
    "\n",
    "There are several ways to go about this. It would be great if we could (partly) automate the generation of ownership tables, using graphs. We could use neo4j for that, or networkx. A query should look something like this:\n",
    "1. For each asset assign a value of 1\n",
    "2. Travel on an ownership relation and multiply the weight of that relationship, e.g. 1\n",
    "3. At the next node find all ownership relationships\n",
    "4. Travel all relationships and multiply by the weight of that relationship, e.g. .5\n",
    "\n",
    "\n",
    "One of the problems with ownership is that we have some ranges and non-precise values (e.g. 75+). A solution is to define lower and upper values and convert them to weights. So this means we would create a few extra columns (percentage_lower_bound, percentage_upper_bound) and convert them to proper percentages so we can easily use them for multiplication. \n",
    "\n",
    "For now we have to assume that a missing percentage is 100 percent. That will often be te case, but we have to go through the Aleph data again one time to fill in the missing percentages.\n",
    "\n",
    "One promising approach is to use Dijkstra's Algorithm, the shortest path, between companies (source) and assets (targets). Because we're dealing with a directed graph, this should omit any detours because companies have joint ventures in other projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean percentages\n",
    "\n",
    "ownerships.percentage = ownerships.percentage.str.replace('+', '-100')\n",
    "ownerships.percentage.fillna('100', inplace=True)\n",
    "ownerships.percentage = ownerships.percentage.astype('str')\n",
    "\n",
    "# Add columns for lower and upper bound\n",
    "\n",
    "ownerships['perc_lower'] = ownerships.percentage.apply(lambda x: float(x.split('-')[0]) / 100 if '-' in x else float(x) / 100)\n",
    "ownerships['perc_higher'] = ownerships.percentage.apply(lambda x: float(x.split('-')[1]) / 100 if '-' in x else float(x) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directed graph\n",
    "\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in assets.iterrows():\n",
    "    G.add_node(row.id,\n",
    "               name=row['name'],\n",
    "               status=row.notes,\n",
    "               country=row.country, \n",
    "               costs=row.amountEur, \n",
    "               mps_uuid=row.description,\n",
    "               aleph_url=row.alephUrl\n",
    "               )\n",
    "    \n",
    "for i, row in companies.iterrows():\n",
    "    G.add_node(row.id,\n",
    "               name=row['name'],\n",
    "               country=row.jurisdiction,\n",
    "               registration=row.registrationNumber,\n",
    "               source_url=row.publisherUrl,\n",
    "               aleph_url=row.alephUrl\n",
    "               )\n",
    "    \n",
    "for i, row in legalentities.iterrows():\n",
    "    G.add_node(row.id,\n",
    "               name=row['name'],\n",
    "               country=row.country,\n",
    "               aleph_url=row.alephUrl)\n",
    "    \n",
    "for i, row in ownerships.iterrows():\n",
    "    G.add_edge(row.owner,\n",
    "               row.asset,\n",
    "               weight_lower=row.perc_lower,\n",
    "               weight_upper=row.perc_higher,\n",
    "               percentage=row.percentage,\n",
    "               source=row.publisherUrl,\n",
    "               aleph_url=row.alephUrl,\n",
    "               id=id,\n",
    "               description=row.description\n",
    "               )\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get source, target and use Dijkstra's algorithm to get the nodes and relationships inbetween\n",
    "\n",
    "def get_ownership(graph: nx.Graph, source: str, target: str, weight_range: str) -> Dict:\n",
    "    '''Calculates ownership percentage\n",
    "    Parameters:\n",
    "    ----------\n",
    "    graph: the graph with assets and owners (G)\n",
    "    source: precise name of PSC/UBO\n",
    "    target: precise name of asset\n",
    "    weight_range: \"lower\" or \"upper\"\n",
    "    '''\n",
    "    \n",
    "    # Get source and target node id\n",
    "    try:\n",
    "        s = [n for n, v in nx.get_node_attributes(graph, 'name').items() if v == source][0]\n",
    "        t = [n for n, v in nx.get_node_attributes(graph, 'name').items() if v == target][0]\n",
    "    except:\n",
    "        return print(f'could not find entry for {source} and {target}')\n",
    "\n",
    "    # Calculate shortest path\n",
    "    try:\n",
    "        [x for x in nx.dijkstra_path(G, s, t)]\n",
    "        shortest_path = [x for x in nx.dijkstra_path(G, s, t)]\n",
    "    except (KeyError, NetworkXNoPath) as error:\n",
    "        #print(f'No path found between {source} and {target}')\n",
    "        ownership = {}\n",
    "        return ownership\n",
    "\n",
    "    # Get node names for reference\n",
    "    nodes = [x for x in shortest_path]\n",
    "    names = [nx.get_node_attributes(G, 'name')[x] for x in nodes]\n",
    "\n",
    "    # Traverse path and get edge weights\n",
    "    window_size = 2\n",
    "    weight = 1\n",
    "    weights = []\n",
    "\n",
    "    if weight_range == 'lower':\n",
    "        weight_range = 'weight_lower'\n",
    "    elif weight_range == 'upper':\n",
    "        weight_range = 'weight_upper'\n",
    "    else:\n",
    "        raise ValueError('weight should be \"lower\" or \"upper\"')\n",
    "\n",
    "    for i in range(len(shortest_path) - window_size + 1):\n",
    "        node1 = shortest_path[i: i + window_size][0]\n",
    "        node2 = shortest_path[i: i + window_size][1]\n",
    "        nodes = list(G.edges([node1, node2], data=True))\n",
    "        for node in nodes:\n",
    "            if node[0] == node1 and node[1] == node2:\n",
    "                e = node[2][weight_range] \n",
    "        weight *= e\n",
    "        weights.append(e)\n",
    "\n",
    "    # Create dict\n",
    "    ownership = {'source_name': source,\n",
    "                 'source_id': s,\n",
    "                 'target_name': target,\n",
    "                 'target_id': t,\n",
    "                 'name_chain': \" -> \".join(names),\n",
    "                 'edge_weights': weights,\n",
    "                 'ownership_percentage': weight}\n",
    "\n",
    "    return ownership\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of ultimate beneficial owners\n",
    "\n",
    "ubos = pd.read_csv(PATH_ALEPH + 'psc.csv')\n",
    "ubos = ubos.company.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 'upper'\n",
    "\n",
    "# Create a dataframe\n",
    "\n",
    "o = []\n",
    "for ubo in ubos:\n",
    "    for i, row in assets.iterrows():\n",
    "        ownership = get_ownership(G, ubo, row['name'], bound)\n",
    "        o.append(ownership)\n",
    "\n",
    "o = list(filter(None, o))\n",
    "o_upper = pd.DataFrame(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add capacity to it\n",
    "\n",
    "df = pd.merge(o_upper,\n",
    "              gdf[['name', 'mps_uuid', 'capacity_mw', 'year', 'number_generators', 'status_simplified', 'status', 'country', 'area_km']],\n",
    "              left_on='target_name',\n",
    "              right_on='name',\n",
    "              how='left')\n",
    "\n",
    "# Calculate capacity per company\n",
    "\n",
    "df[f'output_company_mw_{bound}'] = df.ownership_percentage * df.capacity_mw\n",
    "df[f'company_area_km_{bound}'] = df.ownership_percentage * df.area_km\n",
    "\n",
    "# Rename columns\n",
    "\n",
    "df = df.rename(columns={'source_name': 'company',\n",
    "                         'target_name': 'asset',\n",
    "                         'ownership_percentage': f'ownership_perc_{bound}'})\n",
    "\n",
    "# Drop duplicates\n",
    "\n",
    "df = df.drop_duplicates(subset=['company', 'asset']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with company data\n",
    "\n",
    "df_upper = pd.merge(df,\n",
    "              companies[['name', 'jurisdiction']],\n",
    "              left_on = 'company',\n",
    "              right_on = 'name',\n",
    "              how='left')\n",
    "\n",
    "\n",
    "df_upper.drop(['name_x', 'name_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upper = df_upper.rename(columns={'edge_weights': 'edge_weights_upper'})\n",
    "df_lower = df_lower.rename(columns={'edge_weights': 'edge_weights_lower'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com = pd.merge(df_upper,\n",
    "                 df_lower[['company', 'source_id', 'target_id', 'edge_weights_lower', 'output_company_mw_lower', f'company_area_km_lower', f'ownership_perc_lower']].copy(),\n",
    "                 on=['company', 'source_id', 'target_id'],\n",
    "                 how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com.to_csv(PATH_ALEPH + 'company_windfarm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby company\n",
    "\n",
    "coms = df_com.groupby('company').agg({'output_company_mw_lower': 'sum',\n",
    "                                      'output_company_mw_upper': 'sum',\n",
    "                                      'company_area_km_lower': 'sum',\n",
    "                                      'company_area_km_upper': 'sum'})\n",
    "\n",
    "coms['perc_output_lower'] = coms.output_company_mw_lower / coms.output_company_mw_lower.sum() * 100\n",
    "coms['perc_output_upper'] = coms.output_company_mw_upper / coms.output_company_mw_upper.sum() * 100\n",
    "\n",
    "coms['perc_area_lower'] = coms.company_area_km_lower / coms.company_area_km_lower.sum() * 100\n",
    "coms['perc_area_upper'] = coms.company_area_km_upper / coms.company_area_km_upper.sum() * 100\n",
    "\n",
    "coms.reset_index(inplace=True)\n",
    "\n",
    "coms.to_csv(PATH_ALEPH + f'north_sea_company_output_lower_and_upper_bounds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby company, country and status\n",
    "\n",
    "country = df_com.groupby(['country', 'company', 'status', 'jurisdiction']).agg({'output_company_mw_lower': 'sum',\n",
    "                                                                            'output_company_mw_upper': 'sum',\n",
    "                                                                            'company_area_km_lower': 'sum',\n",
    "                                                                            'company_area_km_upper': 'sum'})\n",
    "\n",
    "country.reset_index(inplace=True)\n",
    "\n",
    "country.to_csv(PATH_ALEPH + f'north_sea_total_output_grouped_by_country_and_company_and_status_lower_and_upper_bounds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts.amount = contracts.amount.str.replace(' MW', '').apply(lambda x: str(x).split(' - ')[0]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = pd.merge(contracts,\n",
    "                     contractawards[['contract', 'supplier']].copy(),\n",
    "                     left_on = 'id', \n",
    "                     right_on = 'contract',\n",
    "                     how='left')\n",
    "\n",
    "len(contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = pd.merge(contracts,\n",
    "                     companies[['name', 'jurisdiction', 'id']].copy(),\n",
    "                     left_on='supplier',\n",
    "                     right_on='id',\n",
    "                     how='left'\n",
    "                     )\n",
    "\n",
    "len(contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts.to_csv(PATH_ALEPH + 'ppa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = pd.merge(other,\n",
    "                 assets,\n",
    "                 left_on='subject',\n",
    "                 right_on='id', \n",
    "                 how='left')\n",
    "\n",
    "other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for endpoints (nodes without incoming edges)\n",
    "\n",
    "endpoints = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "\n",
    "for point in endpoints:\n",
    "    node = G.nodes[point]\n",
    "    if 'country' in node.keys():\n",
    "        \n",
    "        print(node['name'], node['country'])\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
